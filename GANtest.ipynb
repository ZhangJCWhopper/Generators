{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xavier Glorot init\n",
    "def glorot_init(shape):\n",
    "    return tf.random_normal(shape=shape, stddev=1. / tf.sqrt(shape[0]/2.))\n",
    "\n",
    "def readivs(sourceD):\n",
    "    vals = []\n",
    "    indices = []\n",
    "    sourceFList = [one for one in os.listdir(sourceD) if one.endswith(\".ark\") and one.startswith(\"ivector\")]\n",
    "    for sourceF in sourceFList:\n",
    "        with open(os.path.join(sourceD,sourceF)) as s:\n",
    "            for line in s:\n",
    "                step1 = line.split(\"[\")\n",
    "                index = step1[0]\n",
    "                indices.append(index)\n",
    "                content = step1[1].split(\"]\")[0]\n",
    "                values = np.array(map(float, content.split()))\n",
    "                vals.append(values)\n",
    "    #index is seperated by -, speakerid-file(channelid)\n",
    "    return indices, np.array(vals)\n",
    "\n",
    "sourceD = \"/work/jiacen/exp/ivectors_train_female\"\n",
    "indices, ivs = readivs(sourceD)\n",
    "global batch_start, dataset_size\n",
    "batch_start = 0\n",
    "dataset_size = ivs.shape[0]\n",
    "\n",
    "def next_batch(batch_size):\n",
    "    global batch_start, dataset_size\n",
    "    if batch_start + batch_size > dataset_size:\n",
    "        result = np.append(ivs[batch_start:], ivs[:(batch_start+batch_size)%dataset_size],axis=0)\n",
    "        batch_start = (batch_start+batch_size)%dataset_size\n",
    "        return result\n",
    "    result = ivs[batch_start: batch_size + batch_start]\n",
    "    batch_start += batch_size\n",
    "    return result\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training Params\n",
    "num_steps = 100000\n",
    "batch_size = 128\n",
    "learning_rate = 0.0002\n",
    "\n",
    "#Network Params\n",
    "input_dim = 400\n",
    "gen_hidden_dim = 256\n",
    "disc_hidden_dim = 256\n",
    "noise_dim = 100\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'gen_hidden1': tf.Variable(glorot_init([noise_dim, gen_hidden_dim])),\n",
    "    'gen_out': tf.Variable(glorot_init([gen_hidden_dim, input_dim])),\n",
    "    'disc_hidden1': tf.Variable(glorot_init([input_dim, disc_hidden_dim])),\n",
    "    'disc_out': tf.Variable(glorot_init([disc_hidden_dim, 1])),\n",
    "}\n",
    "biases = {\n",
    "    'gen_hidden1': tf.Variable(tf.zeros([gen_hidden_dim])),\n",
    "    'gen_out': tf.Variable(tf.zeros([input_dim])),\n",
    "    'disc_hidden1': tf.Variable(tf.zeros([disc_hidden_dim])),\n",
    "    'disc_out': tf.Variable(tf.zeros([1])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generator\n",
    "def generator(x):\n",
    "    hidden_layer = tf.matmul(x, weights['gen_hidden1'])\n",
    "    hidden_layer = tf.add(hidden_layer, biases['disc_hidden1'])\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    out_layer = tf.matmul(hidden_layer, weights['gen_out'])\n",
    "    out_layer = tf.add(out_layer, biases['gen_out'])\n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    return out_layer\n",
    "\n",
    "#Discriminator\n",
    "def discriminator(x):\n",
    "    hidden_layer = tf.matmul(x, weights['disc_hidden1'])\n",
    "    hidden_layer = tf.add(hidden_layer, biases['disc_hidden1'])\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    out_layer = tf.matmul(hidden_layer, weights['disc_out'])\n",
    "    out_layer = tf.add(out_layer, biases['disc_out'])\n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    return out_layer\n",
    "\n",
    "#Build Networks\n",
    "\n",
    "#Inputs\n",
    "gen_input = tf.placeholder(tf.float32, shape=[None, noise_dim], name='input_noise')\n",
    "disc_input = tf.placeholder(tf.float32, shape=[None, input_dim], name='disc_input')\n",
    "\n",
    "#Generator Network\n",
    "gen_sample = generator(gen_input)\n",
    "\n",
    "disc_real = discriminator(disc_input)\n",
    "disc_fake = discriminator(gen_sample)\n",
    "\n",
    "#Build loss\n",
    "gen_loss = -tf.reduce_mean(tf.log(disc_fake))\n",
    "disc_loss = -tf.reduce_mean(tf.log(disc_real) + tf.log(1.-disc_fake))\n",
    "\n",
    "#Build optimizers\n",
    "optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generator Variables\n",
    "gen_vars = [weights['gen_hidden1'], weights['gen_out'],\n",
    "            biases['gen_hidden1'], biases['gen_out']]\n",
    "\n",
    "#Discriminator Variables\n",
    "disc_vars = [weights['disc_hidden1'], weights['disc_out'],\n",
    "             biases['disc_hidden1'], biases['disc_out']]\n",
    "\n",
    "#Create training operations\n",
    "train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
    "train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\n",
    "\n",
    "#Init the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Generator Loss: 0.603715, Discriminator Loss: 1.401730\n",
      "Step 1000: Generator Loss: 2.055157, Discriminator Loss: 0.145031\n",
      "Step 2000: Generator Loss: 4.028773, Discriminator Loss: 0.020532\n",
      "Step 3000: Generator Loss: 5.147809, Discriminator Loss: 0.008191\n",
      "Step 4000: Generator Loss: 6.629990, Discriminator Loss: 0.001562\n",
      "Step 5000: Generator Loss: 7.652812, Discriminator Loss: 0.002592\n",
      "Step 6000: Generator Loss: 6.994445, Discriminator Loss: 0.001090\n",
      "Step 7000: Generator Loss: 8.699587, Discriminator Loss: 0.000228\n",
      "Step 8000: Generator Loss: 8.725765, Discriminator Loss: 0.000180\n",
      "Step 9000: Generator Loss: 9.647922, Discriminator Loss: 0.000679\n",
      "Step 10000: Generator Loss: 10.114403, Discriminator Loss: 0.000042\n",
      "Step 11000: Generator Loss: 8.557441, Discriminator Loss: 0.000225\n",
      "Step 12000: Generator Loss: 9.775224, Discriminator Loss: 0.000062\n",
      "Step 13000: Generator Loss: 9.052563, Discriminator Loss: 0.000126\n",
      "Step 14000: Generator Loss: 11.353951, Discriminator Loss: 0.000014\n",
      "Step 15000: Generator Loss: 12.162605, Discriminator Loss: 0.000019\n",
      "Step 16000: Generator Loss: 12.547459, Discriminator Loss: 0.000004\n",
      "Step 17000: Generator Loss: 12.744747, Discriminator Loss: 0.000003\n",
      "Step 18000: Generator Loss: 12.912438, Discriminator Loss: 0.000010\n",
      "Step 19000: Generator Loss: 12.346875, Discriminator Loss: 0.000024\n",
      "Step 20000: Generator Loss: 11.527384, Discriminator Loss: 0.000010\n",
      "Step 21000: Generator Loss: 13.636827, Discriminator Loss: 0.000001\n",
      "Step 22000: Generator Loss: 13.698359, Discriminator Loss: 0.000001\n",
      "Step 23000: Generator Loss: 14.440089, Discriminator Loss: 0.000001\n",
      "Step 24000: Generator Loss: 15.061398, Discriminator Loss: 0.000000\n",
      "Step 25000: Generator Loss: 14.990630, Discriminator Loss: 0.000000\n",
      "Step 26000: Generator Loss: 16.550503, Discriminator Loss: 0.000000\n",
      "Step 27000: Generator Loss: 15.740663, Discriminator Loss: 0.000000\n",
      "Step 28000: Generator Loss: 16.876587, Discriminator Loss: 0.000000\n",
      "Step 29000: Generator Loss: 18.307154, Discriminator Loss: 0.000000\n",
      "Step 30000: Generator Loss: 18.167250, Discriminator Loss: 0.000000\n",
      "Step 31000: Generator Loss: 18.663849, Discriminator Loss: 0.000000\n",
      "Step 32000: Generator Loss: 18.930128, Discriminator Loss: 0.000000\n",
      "Step 33000: Generator Loss: 18.473228, Discriminator Loss: -0.000000\n",
      "Step 34000: Generator Loss: 19.363176, Discriminator Loss: 0.000000\n",
      "Step 35000: Generator Loss: 18.697618, Discriminator Loss: 0.000000\n",
      "Step 36000: Generator Loss: 19.066570, Discriminator Loss: -0.000000\n",
      "Step 37000: Generator Loss: 19.797850, Discriminator Loss: 0.000000\n",
      "Step 38000: Generator Loss: 20.112823, Discriminator Loss: 0.000000\n",
      "Step 39000: Generator Loss: 19.463570, Discriminator Loss: 0.000000\n",
      "Step 40000: Generator Loss: 20.291599, Discriminator Loss: -0.000000\n",
      "Step 41000: Generator Loss: 19.312307, Discriminator Loss: 0.000000\n",
      "Step 42000: Generator Loss: 20.168499, Discriminator Loss: -0.000000\n",
      "Step 43000: Generator Loss: 20.811649, Discriminator Loss: -0.000000\n",
      "Step 44000: Generator Loss: 20.305351, Discriminator Loss: -0.000000\n",
      "Step 45000: Generator Loss: 19.402477, Discriminator Loss: -0.000000\n",
      "Step 46000: Generator Loss: 20.796535, Discriminator Loss: -0.000000\n",
      "Step 47000: Generator Loss: 20.526691, Discriminator Loss: 0.000000\n",
      "Step 48000: Generator Loss: 21.038467, Discriminator Loss: -0.000000\n",
      "Step 49000: Generator Loss: 20.573444, Discriminator Loss: -0.000000\n",
      "Step 50000: Generator Loss: 21.133560, Discriminator Loss: -0.000000\n",
      "Step 51000: Generator Loss: 21.955511, Discriminator Loss: 0.000000\n",
      "Step 52000: Generator Loss: 21.839355, Discriminator Loss: -0.000000\n",
      "Step 53000: Generator Loss: 21.258726, Discriminator Loss: -0.000000\n",
      "Step 54000: Generator Loss: 22.373367, Discriminator Loss: -0.000000\n",
      "Step 55000: Generator Loss: 21.914864, Discriminator Loss: -0.000000\n",
      "Step 56000: Generator Loss: 21.420698, Discriminator Loss: 0.000000\n",
      "Step 57000: Generator Loss: 20.692753, Discriminator Loss: -0.000000\n",
      "Step 58000: Generator Loss: 21.727242, Discriminator Loss: -0.000000\n",
      "Step 59000: Generator Loss: 21.767126, Discriminator Loss: -0.000000\n",
      "Step 60000: Generator Loss: 22.066238, Discriminator Loss: -0.000000\n",
      "Step 61000: Generator Loss: 22.207928, Discriminator Loss: -0.000000\n",
      "Step 62000: Generator Loss: 20.610733, Discriminator Loss: -0.000000\n",
      "Step 63000: Generator Loss: 22.411240, Discriminator Loss: -0.000000\n",
      "Step 64000: Generator Loss: 20.803692, Discriminator Loss: 0.000000\n",
      "Step 65000: Generator Loss: 22.829273, Discriminator Loss: -0.000000\n",
      "Step 66000: Generator Loss: 22.895508, Discriminator Loss: -0.000000\n",
      "Step 67000: Generator Loss: 22.408300, Discriminator Loss: -0.000000\n",
      "Step 68000: Generator Loss: 21.700008, Discriminator Loss: -0.000000\n",
      "Step 69000: Generator Loss: 22.319311, Discriminator Loss: -0.000000\n",
      "Step 70000: Generator Loss: 21.732895, Discriminator Loss: -0.000000\n",
      "Step 71000: Generator Loss: 22.853849, Discriminator Loss: -0.000000\n",
      "Step 72000: Generator Loss: 23.347000, Discriminator Loss: 0.000000\n",
      "Step 73000: Generator Loss: 22.817602, Discriminator Loss: -0.000000\n",
      "Step 74000: Generator Loss: 21.803705, Discriminator Loss: -0.000000\n",
      "Step 75000: Generator Loss: 22.929983, Discriminator Loss: -0.000000\n",
      "Step 76000: Generator Loss: 22.333946, Discriminator Loss: -0.000000\n",
      "Step 77000: Generator Loss: 22.713905, Discriminator Loss: -0.000000\n",
      "Step 78000: Generator Loss: 23.293221, Discriminator Loss: -0.000000\n",
      "Step 79000: Generator Loss: 23.202425, Discriminator Loss: -0.000000\n",
      "Step 80000: Generator Loss: 23.577461, Discriminator Loss: -0.000000\n",
      "Step 81000: Generator Loss: 23.872028, Discriminator Loss: -0.000000\n",
      "Step 82000: Generator Loss: 23.472507, Discriminator Loss: -0.000000\n",
      "Step 83000: Generator Loss: 23.744724, Discriminator Loss: -0.000000\n",
      "Step 84000: Generator Loss: 23.961246, Discriminator Loss: -0.000000\n",
      "Step 85000: Generator Loss: 24.144737, Discriminator Loss: -0.000000\n",
      "Step 86000: Generator Loss: 24.296761, Discriminator Loss: -0.000000\n",
      "Step 87000: Generator Loss: 23.252682, Discriminator Loss: -0.000000\n",
      "Step 88000: Generator Loss: 22.941069, Discriminator Loss: -0.000000\n",
      "Step 89000: Generator Loss: 22.616261, Discriminator Loss: -0.000000\n",
      "Step 90000: Generator Loss: 22.355633, Discriminator Loss: -0.000000\n",
      "Step 91000: Generator Loss: 22.888809, Discriminator Loss: -0.000000\n",
      "Step 92000: Generator Loss: 23.216866, Discriminator Loss: -0.000000\n",
      "Step 93000: Generator Loss: 23.447058, Discriminator Loss: -0.000000\n",
      "Step 94000: Generator Loss: 23.081392, Discriminator Loss: -0.000000\n",
      "Step 95000: Generator Loss: 23.319981, Discriminator Loss: -0.000000\n",
      "Step 96000: Generator Loss: 23.499815, Discriminator Loss: -0.000000\n",
      "Step 97000: Generator Loss: 23.651932, Discriminator Loss: -0.000000\n",
      "Step 98000: Generator Loss: 23.792240, Discriminator Loss: -0.000000\n",
      "Step 99000: Generator Loss: 23.897102, Discriminator Loss: -0.000000\n",
      "Step 100000: Generator Loss: 23.991518, Discriminator Loss: -0.000000\n",
      "[array([[  3.43216559e-17,   1.75739009e-18,   3.48726853e-15, ...,\n",
      "          1.73671305e-15,   2.80432174e-16,   2.02680055e-16],\n",
      "       [  1.97069584e-15,   1.37819871e-16,   2.51503815e-13, ...,\n",
      "          1.39099793e-14,   3.13546165e-15,   1.06868929e-15],\n",
      "       [  4.44196317e-15,   4.23462158e-16,   4.34333396e-13, ...,\n",
      "          1.04202650e-13,   1.41715579e-15,   2.46887573e-15],\n",
      "       [  9.79753862e-15,   6.43571017e-15,   3.86015803e-11, ...,\n",
      "          3.24491562e-12,   3.37919186e-14,   8.07154213e-15]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(1, num_steps+1):\n",
    "        #Prepare data\n",
    "        #Get the next batch of data\n",
    "        batch_x = next_batch(batch_size)\n",
    "        #Generate noise\n",
    "        z = np.random .uniform(-1., 1., size=[batch_size, noise_dim])\n",
    "        \n",
    "        #Train\n",
    "        feed_dict = {disc_input: batch_x, gen_input: z}\n",
    "        _, _, gl, dl = sess.run([train_gen, train_disc, gen_loss, disc_loss],\n",
    "                               feed_dict=feed_dict)\n",
    "        if i % 1000 == 0 or i == 1:\n",
    "            print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (i, gl, dl))\n",
    "    z = np.random.uniform(-1., 1., size=[4, noise_dim])\n",
    "    g = sess.run([gen_sample], feed_dict={gen_input: z})\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
